#!/usr/bin/env python3
"""
Production TTA.dev API Server with Real AI Integration
Includes: Gemini LLM, TTA.dev Primitives (Cache, Retry, Fallback)
"""

import os
import sys
import time
import asyncio
import uuid
from typing import Any
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Import uvicorn
try:
    import uvicorn
except ImportError:
    print("‚ùå Error: uvicorn not installed. Run: uv pip install uvicorn")
    sys.exit(1)

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    # Load from TTA.dev root directory
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)
    print(f"‚úÖ Loaded environment variables from: {env_path}")
except ImportError:
    print("‚ö†Ô∏è  python-dotenv not installed. Install with: uv pip install python-dotenv")
    print("‚ö†Ô∏è  Falling back to system environment variables")

# Add packages to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../packages/tta-dev-primitives/src"))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../packages/tta-rebuild/src"))

# Import TTA.dev primitives
try:
    from tta_dev_primitives import WorkflowContext, WorkflowPrimitive
    from tta_dev_primitives.recovery import RetryPrimitive, RetryStrategy, FallbackPrimitive
    from tta_dev_primitives.performance import CachePrimitive
    TTA_PRIMITIVES_AVAILABLE = True
    print("‚úÖ TTA.dev primitives loaded")

    # Import Gemini SDK
    try:
        import google.generativeai as genai
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini API key found")
    except ImportError:
        print("‚ö†Ô∏è google-generativeai not installed. Run: uv pip install google-generativeai")
        GEMINI_AVAILABLE = False

    # Use the real WorkflowPrimitive for our classes
    class GeminiLLMPrimitive(WorkflowPrimitive[dict, dict]):
        """Gemini LLM primitive that uses google-generativeai SDK directly."""

        def __init__(self, model_name: str = "gemini-2.5-flash"):
            """Initialize with Gemini model."""
            super().__init__()
            self.model_name = model_name

            # Configure Gemini API
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY environment variable not set")

            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)

        async def execute(self, input_data: dict, context: WorkflowContext) -> dict:
            """Execute LLM generation."""
            prompt = input_data.get("prompt", "")
            temperature = input_data.get("temperature", 0.7)
            max_tokens = input_data.get("max_tokens", 2048)

            # Generate with Gemini
            try:
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    prompt,
                    generation_config={
                        "temperature": temperature,
                        "max_output_tokens": max_tokens
                    }
                )

                # Extract text
                text = response.text if hasattr(response, 'text') else str(response)

                # Estimate tokens (rough approximation)
                prompt_tokens = len(prompt.split()) * 1.3
                completion_tokens = len(text.split()) * 1.3
                total_tokens = int(prompt_tokens + completion_tokens)

                # Estimate cost (Gemini Flash pricing)
                cost_per_1m_input = 0.15  # $0.15 per 1M input tokens
                cost_per_1m_output = 0.60  # $0.60 per 1M output tokens
                estimated_cost = (
                    (prompt_tokens / 1_000_000 * cost_per_1m_input) +
                    (completion_tokens / 1_000_000 * cost_per_1m_output)
                )

                return {
                    "response": text,
                    "model_used": self.model_name,
                    "tokens_used": total_tokens,
                    "estimated_cost_usd": round(estimated_cost, 6)
                }

            except Exception as e:
                raise Exception(f"Gemini API error: {str(e)}")

    class MockLLMPrimitive(WorkflowPrimitive[dict, dict]):
        """Mock LLM for when Gemini is not available."""

        async def execute(self, input_data: dict, context: WorkflowContext) -> dict:
            """Return mock response."""
            prompt = input_data.get("prompt", "")
            return {
                "response": f"[MOCK] Analysis of: {prompt[:100]}... This is a simulated response for testing purposes.",
                "model_used": "mock-llm",
                "tokens_used": 50,
                "estimated_cost_usd": 0.0
            }

except ImportError as e:
    print(f"‚ö†Ô∏è TTA primitives not available: {e}")
    TTA_PRIMITIVES_AVAILABLE = False

    # Minimal mocks when primitives not available
    class WorkflowContext:
        def __init__(self, correlation_id=None, data=None):
            self.correlation_id = correlation_id or str(uuid.uuid4())
            self.data = data or {}

    class WorkflowPrimitive:
        """Mock WorkflowPrimitive for when tta-dev-primitives not available."""
        def __init__(self):
            pass

        async def execute(self, input_data: dict, context: Any) -> dict:
            return {"response": "TTA primitives not available", "model_used": "none", "tokens_used": 0, "estimated_cost_usd": 0.0}

    class GeminiLLMPrimitive(WorkflowPrimitive):
        def __init__(self, model_name: str = "gemini-2.5-flash"):
            super().__init__()
            self.model_name = model_name

    class MockLLMPrimitive(WorkflowPrimitive):
        async def execute(self, input_data: dict, context: Any) -> dict:
            return {"response": "Mock response", "model_used": "mock", "tokens_used": 50, "estimated_cost_usd": 0.0}

# Import Gemini directly (not using tta-rebuild)
GEMINI_AVAILABLE = False
try:
    import google.generativeai as genai

    api_key = os.getenv("GEMINI_API_KEY")
    if api_key:
        genai.configure(api_key=api_key)
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini API key found")
    else:
        print("‚ö†Ô∏è GEMINI_API_KEY not set - using mock mode")
except ImportError:
    print("‚ö†Ô∏è  google-generativeai not installed - using mock mode")


# Initialize FastAPI app
app = FastAPI(
    title="TTA.dev API - Production Ready",
    description="Real AI analysis with TTA.dev workflow primitives",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response Models
class AnalyzeRequest(BaseModel):
    """Request to analyze repository or text data"""
    prompt: str = Field(..., description="Analysis prompt or question")
    context: dict[str, Any] = Field(default_factory=dict, description="Additional context data")
    model: str = Field(default="gemini-1.5-flash", description="LLM model to use")
    temperature: float = Field(default=0.7, ge=0.0, le=1.0, description="Creativity level")
    use_cache: bool = Field(default=True, description="Enable caching")
    max_retries: int = Field(default=3, ge=0, le=10, description="Retry attempts")


class AnalyzeResponse(BaseModel):
    """Response from analysis"""
    success: bool
    response: str | None = None
    error: str | None = None
    execution_time_ms: float
    cache_hit: bool = False
    model_used: str
    correlation_id: str
    tokens_used: int = 0
    estimated_cost_usd: float = 0.0


class HealthResponse(BaseModel):
    """API health check response"""
    status: str
    gemini_available: bool
    primitives_loaded: bool
    version: str
    cache_enabled: bool


# LLM Primitive implementations (using the new GeminiLLMPrimitive defined above)
class MockLLMPrimitive:
    """Mock LLM primitive for when Gemini unavailable."""

    async def execute(self, input_data: dict[str, Any], context: Any) -> dict[str, Any]:
        """Execute mock LLM analysis."""
        prompt = input_data.get("prompt", "")

        analysis = f"""
ü§ñ MOCK MODE - Gemini API key not configured

Based on your prompt:
{prompt}

This is a demonstration response. To enable real AI analysis:
1. Get a free Gemini API key: https://ai.google.dev/
2. Set environment variable: export GEMINI_API_KEY='your_key_here'  # pragma: allowlist secret
3. Restart this API server

The prompt was received successfully and the workflow is operational.
In production mode, this would be analyzed by Gemini 1.5 Flash with:
- Automatic retry on failures (RetryPrimitive)
- LRU caching for cost reduction (CachePrimitive)
- Fallback to alternative models (FallbackPrimitive)
- Full distributed tracing (OpenTelemetry)
""".strip()

        return {
            "analysis": analysis,
            "model_used": "mock-demo",
            "tokens_used": len(prompt.split()) * 2,
            "estimated_cost_usd": 0.0,
        }


# Initialize LLM (with fallback to mock)
if GEMINI_AVAILABLE:
    try:
        llm_primitive = GeminiLLMPrimitive()
        print("‚úÖ Using real Gemini LLM")
    except Exception as e:
        print(f"‚ö†Ô∏è  Gemini initialization failed: {e}")
        llm_primitive = MockLLMPrimitive()
        print("‚úÖ Using mock LLM")
        GEMINI_AVAILABLE = False
else:
    llm_primitive = MockLLMPrimitive()
    print("‚úÖ Using mock LLM (Gemini not available)")


# Apply TTA.dev primitives if available
if TTA_PRIMITIVES_AVAILABLE and GEMINI_AVAILABLE:
    print("üîß Applying TTA.dev primitives...")

    # Layer 1: Cache (40-60% cost reduction)
    cached_llm = CachePrimitive(
        primitive=llm_primitive,
        cache_key_fn=lambda data, ctx: f"{data.get('prompt', '')}:{data.get('temperature', 0.7)}",
        ttl_seconds=3600.0  # 1 hour cache
    )
    print("  ‚úÖ CachePrimitive (1 hour TTL)")

    # Layer 2: Retry (resilience)
    resilient_llm = RetryPrimitive(
        primitive=cached_llm,
        strategy=RetryStrategy(
            max_retries=3,
            backoff_base=2.0,
            jitter=True
        )
    )
    print("  ‚úÖ RetryPrimitive (3 retries, exponential backoff)")

    # Use the wrapped primitive
    workflow_primitive = resilient_llm
    CACHE_ENABLED = True
    print("‚úÖ Production primitives active")
else:
    # Direct LLM call (no primitives)
    workflow_primitive = llm_primitive
    CACHE_ENABLED = False
    print("‚ö†Ô∏è  Using direct LLM (no primitives)")


@app.get("/", response_model=HealthResponse)
async def root():
    """Root endpoint with health info"""
    return HealthResponse(
        status="healthy",
        gemini_available=GEMINI_AVAILABLE,
        primitives_loaded=TTA_PRIMITIVES_AVAILABLE,
        version="2.0.0",
        cache_enabled=CACHE_ENABLED
    )


@app.get("/health", response_model=HealthResponse)
async def health():
    """Health check endpoint"""
    return await root()


@app.post("/api/v1/analyze", response_model=AnalyzeResponse)
async def analyze(request: AnalyzeRequest) -> AnalyzeResponse:
    """
    Analyze data using real Gemini AI + TTA.dev primitives.

    Production features:
    - Real Gemini 1.5 Flash analysis
    - Automatic retry on failures (RetryPrimitive)
    - LRU caching for cost reduction (CachePrimitive)
    - Distributed tracing (correlation IDs)
    - Cost tracking
    """
    start_time = time.time()
    correlation_id = str(uuid.uuid4())
    context = WorkflowContext(correlation_id=correlation_id, data=request.context)

    try:
        # Execute with primitives
        result = await workflow_primitive.execute(
            {
                "prompt": request.prompt,
                "temperature": request.temperature,
                "context": request.context
            },
            context
        )

        execution_time_ms = (time.time() - start_time) * 1000

        return AnalyzeResponse(
            success=True,
            response=result["analysis"],
            execution_time_ms=execution_time_ms,
            cache_hit=False,  # Would need to track this from CachePrimitive
            model_used=result.get("model_used", request.model),
            correlation_id=correlation_id,
            tokens_used=result.get("tokens_used", 0),
            estimated_cost_usd=result.get("estimated_cost_usd", 0.0)
        )

    except Exception as e:
        execution_time_ms = (time.time() - start_time) * 1000

        return AnalyzeResponse(
            success=False,
            error=str(e),
            execution_time_ms=execution_time_ms,
            model_used=request.model,
            correlation_id=correlation_id
        )


@app.post("/api/v1/github/analyze", response_model=AnalyzeResponse)
async def analyze_github_repo(request: AnalyzeRequest) -> AnalyzeResponse:
    """
    Analyze GitHub repository data.

    Expects request.context to contain GitHub repo stats.
    """
    # Extract GitHub stats from context
    repo_data = request.context

    # Enhance prompt with GitHub data if available
    enhanced_prompt = f"""
{request.prompt}

Repository Data:
- Name: {repo_data.get('full_name', 'N/A')}
- Stars: {repo_data.get('stargazers_count', 0)}
- Forks: {repo_data.get('forks_count', 0)}
- Open Issues: {repo_data.get('open_issues_count', 0)}
- Language: {repo_data.get('language', 'N/A')}
- Description: {repo_data.get('description', 'N/A')}
- Created: {repo_data.get('created_at', 'N/A')}
- Updated: {repo_data.get('updated_at', 'N/A')}

Provide detailed analysis in a structured format.
""".strip()

    # Use the standard analyze endpoint
    enhanced_request = AnalyzeRequest(
        prompt=enhanced_prompt,
        context=repo_data,
        model=request.model,
        temperature=request.temperature,
        use_cache=request.use_cache,
        max_retries=request.max_retries
    )

    return await analyze(enhanced_request)


@app.get("/api/v1/primitives")
async def list_primitives():
    """List active TTA.dev primitives"""
    primitives = []

    if TTA_PRIMITIVES_AVAILABLE:
        primitives.append({
            "name": "CachePrimitive",
            "enabled": CACHE_ENABLED,
            "description": "LRU cache with 1 hour TTL",
            "config": {"ttl_seconds": 3600, "max_size": 1000}
        })
        primitives.append({
            "name": "RetryPrimitive",
            "enabled": CACHE_ENABLED,
            "description": "Exponential backoff retry",
            "config": {"max_retries": 3, "strategy": "exponential"}
        })

    return {
        "primitives": primitives,
        "total_enabled": len([p for p in primitives if p["enabled"]])
    }


if __name__ == "__main__":
    print("\n" + "="*60)
    print("üöÄ TTA.dev API Server - Production Ready")
    print("="*60)
    print(f"Gemini: {'‚úÖ ENABLED' if GEMINI_AVAILABLE else '‚ùå DISABLED (using mock)'}")
    print(f"Primitives: {'‚úÖ ACTIVE' if TTA_PRIMITIVES_AVAILABLE else '‚ùå INACTIVE'}")
    print(f"Cache: {'‚úÖ ENABLED' if CACHE_ENABLED else '‚ùå DISABLED'}")
    print("="*60)
    print("\nüìç Starting server on http://localhost:8000")
    print("üìñ API docs: http://localhost:8000/docs")
    print("\n")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
