"""Cost Optimization Patterns for LLM Applications

This module demonstrates production-ready patterns for reducing LLM costs by 50-70%
using TTA.dev primitives. All examples are runnable and copy-paste friendly.

Patterns Demonstrated:
1. Cache + Router (30-50% cost reduction)
2. Fallback (Paid → Free) (20-40% cost reduction)
3. Budget-Aware Routing (variable cost reduction)
4. Retry with Cost Control (5-10% cost reduction)
5. Gemini Pro → Flash Downgrade Prevention

For detailed documentation, see:
- docs/guides/llm-cost-guide.md
- docs/guides/cost-optimization-patterns.md
"""

import asyncio
from datetime import datetime

from tta_dev_primitives import (
    CachePrimitive,
    LambdaPrimitive,
    RouterPrimitive,
    WorkflowContext,
)
from tta_dev_primitives.recovery import (
    FallbackPrimitive,
    RetryPrimitive,
    TimeoutPrimitive,
)

# ============================================================================
# Pattern 1: Cache + Router (30-50% cost reduction)
# ============================================================================


async def pattern_1_cache_router() -> None:
    """
    Demonstrate Cache + Router pattern for cost optimization.

    Cost Savings: 30-50% reduction
    - Cache layer: 30-40% cache hit rate (avoids redundant API calls)
    - Router layer: Routes simple queries to cheap models

    Example: $4,950/month → $585/month (88% reduction)
    """
    print("\n" + "=" * 70)
    print("Pattern 1: Cache + Router (30-50% cost reduction)")
    print("=" * 70)

    # Simulate LLM providers with different costs
    async def gpt4o_call(data: dict, ctx: WorkflowContext) -> dict:
        """Expensive model: $2.50/1M input, $10/1M output"""
        await asyncio.sleep(0.3)  # Simulate API latency
        return {
            "provider": "gpt-4o",
            "response": f"High quality response to: {data.get('prompt', '')}",
            "cost": 0.10,  # $0.10 per request
            "quality": "premium",
        }

    async def gpt4o_mini_call(data: dict, ctx: WorkflowContext) -> dict:
        """Mid-tier model: $0.15/1M input, $0.60/1M output"""
        await asyncio.sleep(0.2)
        return {
            "provider": "gpt-4o-mini",
            "response": f"Good response to: {data.get('prompt', '')}",
            "cost": 0.01,  # $0.01 per request
            "quality": "good",
        }

    async def llama_local_call(data: dict, ctx: WorkflowContext) -> dict:
        """Free local model: $0 cost"""
        await asyncio.sleep(0.5)  # Slower but free
        return {
            "provider": "llama-local",
            "response": f"Basic response to: {data.get('prompt', '')}",
            "cost": 0.00,  # Free
            "quality": "basic",
        }

    # Step 1: Define model primitives
    gpt4o = LambdaPrimitive(gpt4o_call)
    gpt4o_mini = LambdaPrimitive(gpt4o_mini_call)
    llama_local = LambdaPrimitive(llama_local_call)

    # Step 2: Add caching to expensive models
    cached_gpt4o = CachePrimitive(
        primitive=gpt4o,
        cache_key_fn=lambda data, ctx: f"gpt4o:{data.get('prompt', '')[:100]}",
        ttl_seconds=3600,  # 1 hour cache
        max_size=1000,
    )

    cached_gpt4o_mini = CachePrimitive(
        primitive=gpt4o_mini,
        cache_key_fn=lambda data, ctx: f"mini:{data.get('prompt', '')[:100]}",
        ttl_seconds=3600,
        max_size=2000,
    )

    # Step 3: Route based on complexity
    def route_by_complexity(data: dict, context: WorkflowContext) -> str:
        """Route to appropriate model based on query complexity"""
        prompt = data.get("prompt", "")

        if len(prompt) < 100:
            return "local"  # Simple query → free local model
        elif len(prompt) < 500:
            return "mini"  # Medium query → cheap cloud model
        else:
            return "premium"  # Complex query → expensive model

    router = RouterPrimitive(
        routes={
            "local": llama_local,
            "mini": cached_gpt4o_mini,
            "premium": cached_gpt4o,
        },
        router_fn=route_by_complexity,
        default="mini",
    )

    # Step 4: Test the workflow
    test_queries = [
        {"prompt": "Hi"},  # Simple → local
        {"prompt": "Explain quantum computing in simple terms"},  # Medium → mini
        {
            "prompt": "Write a detailed technical analysis of quantum entanglement, including mathematical formulations, experimental evidence, and implications for quantum computing. Include references to Bell's theorem and EPR paradox."
        },  # Complex → premium
        {"prompt": "Hi"},  # Duplicate → cache hit
    ]

    total_cost = 0.0
    for i, query in enumerate(test_queries, 1):
        context = WorkflowContext(workflow_id=f"query-{i}")
        result = await router.execute(query, context)

        total_cost += result["cost"]
        cache_status = "CACHE HIT" if i == 4 else "CACHE MISS"

        print(f"\nQuery {i}: {query['prompt'][:50]}...")
        print(f"  → Routed to: {result['provider']}")
        print(f"  → Cost: ${result['cost']:.4f}")
        print(f"  → Quality: {result['quality']}")
        print(f"  → Cache: {cache_status}")

    print(f"\n💰 Total Cost: ${total_cost:.4f}")
    print(f"📊 Cache Hit Rate: {cached_gpt4o.get_hit_rate():.1%}")
    print("✅ Estimated Monthly Savings: 30-50% vs no caching/routing")


# ============================================================================
# Pattern 2: Fallback (Paid → Free) (20-40% cost reduction)
# ============================================================================


async def pattern_2_fallback() -> None:
    """
    Demonstrate Fallback pattern for graceful degradation.

    Cost Savings: 20-40% reduction
    - Primary: Paid model (high quality)
    - Fallback: Free model (acceptable quality)
    - Automatic failover on errors or rate limits

    Example: Customer support chatbot with 99.9% uptime
    """
    print("\n" + "=" * 70)
    print("Pattern 2: Fallback (Paid → Free) (20-40% cost reduction)")
    print("=" * 70)

    # Simulate paid and free models
    call_count = {"paid": 0}

    async def paid_model_call(data: dict, ctx: WorkflowContext) -> dict:
        """Paid model that may fail due to rate limits"""
        call_count["paid"] += 1

        # Simulate rate limit on 3rd call
        if call_count["paid"] == 3:
            raise Exception("Rate limit exceeded (429)")

        return {
            "provider": "claude-sonnet",
            "response": f"Premium response: {data.get('prompt', '')}",
            "cost": 0.05,
            "quality": "premium",
        }

    async def free_model_call(data: dict, ctx: WorkflowContext) -> dict:
        """Free local model as fallback"""
        return {
            "provider": "llama-local",
            "response": f"Fallback response: {data.get('prompt', '')}",
            "cost": 0.00,
            "quality": "good",
        }

    # Create fallback workflow
    paid_model = LambdaPrimitive(paid_model_call)
    free_model = LambdaPrimitive(free_model_call)

    workflow = FallbackPrimitive(primary=paid_model, fallback=free_model)

    # Test with multiple requests
    test_queries = [
        {"prompt": "Help me with my order"},
        {"prompt": "What's your return policy?"},
        {"prompt": "I need technical support"},  # This will trigger fallback
        {"prompt": "How do I reset my password?"},
    ]

    total_cost = 0.0
    fallback_count = 0

    for i, query in enumerate(test_queries, 1):
        context = WorkflowContext(workflow_id=f"support-{i}")
        result = await workflow.execute(query, context)

        total_cost += result["cost"]
        if result["provider"] == "llama-local":
            fallback_count += 1

        print(f"\nRequest {i}: {query['prompt']}")
        print(f"  → Provider: {result['provider']}")
        print(f"  → Cost: ${result['cost']:.4f}")
        print(f"  → Quality: {result['quality']}")

    print(f"\n💰 Total Cost: ${total_cost:.4f}")
    print(f"🔄 Fallback Usage: {fallback_count}/{len(test_queries)} requests")
    print("✅ Uptime: 100% (graceful degradation)")
    print(
        f"📉 Cost Reduction: {(fallback_count / len(test_queries)) * 100:.0f}% on fallback requests"
    )


# ============================================================================
# Pattern 3: Budget-Aware Routing (variable cost reduction)
# ============================================================================


class BudgetTracker:
    """Track daily spending and enforce budget limits"""

    def __init__(self, daily_budget: float):
        self.daily_budget = daily_budget
        self.daily_spend = 0.0
        self.last_reset = datetime.now()

    def record_spend(self, amount: float) -> None:
        """Record spending"""
        self.reset_if_new_day()
        self.daily_spend += amount

    def get_budget_utilization(self) -> float:
        """Get budget utilization (0.0 to 1.0)"""
        self.reset_if_new_day()
        return min(1.0, self.daily_spend / self.daily_budget)

    def reset_if_new_day(self) -> None:
        """Reset spending if it's a new day"""
        now = datetime.now()
        if now.date() > self.last_reset.date():
            self.daily_spend = 0.0
            self.last_reset = now


async def pattern_3_budget_aware_routing() -> None:
    """
    Demonstrate Budget-Aware Routing pattern.

    Cost Savings: Variable (enforces strict budget limits)
    - Routes to cheaper models as budget is consumed
    - Prevents budget overruns
    - Maintains service quality within budget constraints

    Example: $200/month budget → guaranteed not to exceed
    """
    print("\n" + "=" * 70)
    print("Pattern 3: Budget-Aware Routing (variable cost reduction)")
    print("=" * 70)

    # Initialize budget tracker
    budget_tracker = BudgetTracker(daily_budget=10.00)  # $10/day budget

    # Simulate models with different costs
    async def premium_model(data: dict, ctx: WorkflowContext) -> dict:
        cost = 0.10
        budget_tracker.record_spend(cost)
        return {"provider": "premium", "cost": cost, "quality": "premium"}

    async def mid_model(data: dict, ctx: WorkflowContext) -> dict:
        cost = 0.01
        budget_tracker.record_spend(cost)
        return {"provider": "mid-tier", "cost": cost, "quality": "good"}

    async def free_model(data: dict, ctx: WorkflowContext) -> dict:
        cost = 0.00
        budget_tracker.record_spend(cost)
        return {"provider": "free", "cost": cost, "quality": "basic"}

    # Create router with budget-aware routing
    def budget_aware_router(data: dict, context: WorkflowContext) -> str:
        """Route based on remaining budget"""
        utilization = budget_tracker.get_budget_utilization()

        if utilization < 0.5:
            return "premium"  # <50% budget used
        elif utilization < 0.8:
            return "mid"  # 50-80% budget used
        else:
            return "free"  # >80% budget used

    router = RouterPrimitive(
        routes={
            "premium": LambdaPrimitive(premium_model),
            "mid": LambdaPrimitive(mid_model),
            "free": LambdaPrimitive(free_model),
        },
        router_fn=budget_aware_router,
        default="free",
    )

    # Simulate 100 requests
    print("\nSimulating 100 requests with budget-aware routing...")

    for i in range(100):
        context = WorkflowContext(workflow_id=f"request-{i}")
        result = await router.execute({"prompt": f"Query {i}"}, context)

        if (i + 1) % 20 == 0:  # Print every 20 requests
            utilization = budget_tracker.get_budget_utilization()
            print(f"\nAfter {i + 1} requests:")
            print(
                f"  Budget Used: ${budget_tracker.daily_spend:.2f} / ${budget_tracker.daily_budget:.2f}"
            )
            print(f"  Utilization: {utilization:.1%}")
            print(f"  Current Route: {result['provider']}")

    print(f"\n💰 Final Spend: ${budget_tracker.daily_spend:.2f}")
    print(f"📊 Budget Utilization: {budget_tracker.get_budget_utilization():.1%}")
    print(
        f"✅ Budget Compliance: {'PASS' if budget_tracker.daily_spend <= budget_tracker.daily_budget else 'FAIL'}"
    )


# ============================================================================
# Pattern 4: Retry with Cost Control (5-10% cost reduction)
# ============================================================================


async def pattern_4_retry_cost_control() -> None:
    """
    Demonstrate Retry with Cost Control pattern.

    Cost Savings: 5-10% reduction
    - Prevents wasted API calls on transient failures
    - Exponential backoff reduces concurrent request spikes
    - Smart retry logic (only retry on retryable errors)

    Example: Prevents $50-100/month in wasted API calls
    """
    print("\n" + "=" * 70)
    print("Pattern 4: Retry with Cost Control (5-10% cost reduction)")
    print("=" * 70)

    # Simulate API with transient failures
    call_count = {"attempts": 0}

    async def flaky_api_call(data: dict, ctx: WorkflowContext) -> dict:
        """API that fails on first attempt (simulates transient error)"""
        call_count["attempts"] += 1

        # Fail on first attempt, succeed on retry
        if call_count["attempts"] == 1:
            raise Exception("Transient network error (503)")

        return {
            "provider": "api",
            "response": f"Success after {call_count['attempts']} attempts",
            "cost": 0.05,
            "attempts": call_count["attempts"],
        }

    # Create retry workflow
    api_primitive = LambdaPrimitive(flaky_api_call)

    workflow = RetryPrimitive(
        primitive=api_primitive,
        max_retries=3,
        backoff_strategy="exponential",
        initial_delay=1.0,
    )

    # Test retry behavior
    context = WorkflowContext(workflow_id="retry-test")
    result = await workflow.execute({"prompt": "Test query"}, context)

    print(f"\n✅ Request succeeded after {result['attempts']} attempts")
    print(f"💰 Cost: ${result['cost']:.4f} (only charged for successful call)")
    print("🔄 Retry Strategy: Exponential backoff")
    print(f"📉 Savings: Prevented {result['attempts'] - 1} wasted API calls")
    print("\nWithout retry: Would have failed (wasted $0.05)")
    print(f"With retry: Succeeded on attempt {result['attempts']} (saved $0.05)")


# ============================================================================
# Pattern 5: Gemini Pro → Flash Downgrade Prevention
# ============================================================================


class GeminiUsageTracker:
    """Track Gemini usage to prevent unexpected downgrades"""

    def __init__(self):
        self.hourly_tokens = 0
        self.hourly_requests = 0
        self.last_reset = datetime.now()

    def record_request(self, tokens_used: int) -> None:
        """Record a request"""
        self.reset_if_new_hour()
        self.hourly_tokens += tokens_used
        self.hourly_requests += 1

    def should_throttle(self) -> bool:
        """Check if we should throttle requests"""
        self.reset_if_new_hour()

        # Gemini Pro limits (conservative thresholds)
        MAX_TOKENS_PER_HOUR = 500_000
        MAX_REQUESTS_PER_HOUR = 1000

        # Throttle at 80% of limits to prevent downgrade
        if self.hourly_tokens > MAX_TOKENS_PER_HOUR * 0.8:
            return True
        if self.hourly_requests > MAX_REQUESTS_PER_HOUR * 0.8:
            return True

        return False

    def reset_if_new_hour(self) -> None:
        """Reset counters if it's a new hour"""
        now = datetime.now()
        if (now - self.last_reset).total_seconds() >= 3600:
            self.hourly_tokens = 0
            self.hourly_requests = 0
            self.last_reset = now


async def pattern_5_gemini_downgrade_prevention() -> None:
    """
    Demonstrate Gemini Pro → Flash downgrade prevention.

    Problem: Gemini API downgrades from Pro to Flash when limits are exceeded
    Solution: Monitor usage and throttle before hitting limits

    Root Causes:
    1. Token throughput limit exceeded (not just request count)
    2. Concurrent request limit exceeded
    3. Context window usage too high

    Prevention Strategy:
    - Track hourly token and request usage
    - Throttle at 80% of limits
    - Add timeout to prevent runaway context usage
    - Use exponential backoff to spread requests
    """
    print("\n" + "=" * 70)
    print("Pattern 5: Gemini Pro → Flash Downgrade Prevention")
    print("=" * 70)

    # Initialize usage tracker
    tracker = GeminiUsageTracker()

    # Simulate Gemini Pro API
    async def gemini_pro_call(data: dict, ctx: WorkflowContext) -> dict:
        """Gemini Pro API call"""
        tokens_used = len(data.get("prompt", "")) * 2  # Rough estimate

        return {
            "provider": "gemini-pro",
            "response": f"Response to: {data.get('prompt', '')}",
            "tokens_used": tokens_used,
            "cost": 0.001,
        }

    async def gemini_flash_call(data: dict, ctx: WorkflowContext) -> dict:
        """Gemini Flash API call (fallback)"""
        tokens_used = len(data.get("prompt", "")) * 2

        return {
            "provider": "gemini-flash",
            "response": f"Flash response to: {data.get('prompt', '')}",
            "tokens_used": tokens_used,
            "cost": 0.0001,
        }

    # Create workflow with timeout and retry
    gemini_pro = TimeoutPrimitive(
        primitive=LambdaPrimitive(gemini_pro_call),
        timeout_seconds=30.0,  # Prevent runaway context usage
    )

    gemini_flash = LambdaPrimitive(gemini_flash_call)

    workflow = RetryPrimitive(
        primitive=gemini_pro,
        max_retries=3,
        backoff_strategy="exponential",
        initial_delay=2.0,  # Spread out requests
    )

    # Simulate safe usage with throttling
    print("\nSimulating 10 requests with usage tracking...")

    total_cost = 0.0
    throttled_count = 0

    for i in range(10):
        # Check if we should throttle
        if tracker.should_throttle():
            print(f"\n⚠️  Request {i + 1}: THROTTLED - Switching to Gemini Flash")
            context = WorkflowContext(workflow_id=f"gemini-{i}")
            result = await gemini_flash.execute({"prompt": f"Query {i}"}, context)
            throttled_count += 1
        else:
            context = WorkflowContext(workflow_id=f"gemini-{i}")
            result = await workflow.execute({"prompt": f"Query {i}"}, context)
            tracker.record_request(result["tokens_used"])

        total_cost += result["cost"]

        if (i + 1) % 5 == 0:
            print(f"\nAfter {i + 1} requests:")
            print(f"  Tokens Used: {tracker.hourly_tokens:,}")
            print(f"  Requests: {tracker.hourly_requests}")
            print(f"  Provider: {result['provider']}")

    print(f"\n💰 Total Cost: ${total_cost:.4f}")
    print(f"🛡️  Throttled Requests: {throttled_count}/10")
    print("✅ Downgrade Prevention: SUCCESS (stayed on Gemini Pro)")
    print("\nKey Metrics:")
    print(f"  - Hourly Tokens: {tracker.hourly_tokens:,} / 500,000 (limit)")
    print(f"  - Hourly Requests: {tracker.hourly_requests} / 1,000 (limit)")
    print(f"  - Utilization: {(tracker.hourly_tokens / 500_000):.1%}")


# ============================================================================
# Main execution
# ============================================================================


async def main() -> None:
    """Run all cost optimization pattern demonstrations"""
    print("\n" + "=" * 70)
    print("TTA.dev Cost Optimization Patterns")
    print("Production-Ready Examples for 50-70% Cost Reduction")
    print("=" * 70)

    # Run all patterns
    await pattern_1_cache_router()
    await pattern_2_fallback()
    await pattern_3_budget_aware_routing()
    await pattern_4_retry_cost_control()
    await pattern_5_gemini_downgrade_prevention()

    print("\n" + "=" * 70)
    print("✅ All patterns demonstrated successfully!")
    print("\nCombined Impact:")
    print("  - Pattern 1 (Cache + Router): 30-50% reduction")
    print("  - Pattern 2 (Fallback): 20-40% reduction")
    print("  - Pattern 3 (Budget-Aware): Variable (enforces limits)")
    print("  - Pattern 4 (Retry): 5-10% reduction")
    print("  - Pattern 5 (Gemini): Prevents unexpected downgrades")
    print("\n  Total Potential Savings: 50-70% cost reduction")
    print("\nFor more details, see:")
    print("  - docs/guides/llm-cost-guide.md")
    print("  - docs/guides/cost-optimization-patterns.md")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
