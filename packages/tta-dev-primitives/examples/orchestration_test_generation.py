"""Automated Test Generation with Multi-Model Orchestration.

Demonstrates a production-ready workflow that uses Claude Sonnet 4.5 as an orchestrator
to analyze code and delegate test generation to Gemini Pro, achieving 90%+ cost savings
while maintaining quality.

**Workflow:**
1. Claude analyzes code structure and requirements
2. Claude creates detailed test generation plan
3. Gemini Pro generates unit tests (bulk execution, free)
4. Claude validates test quality and coverage
5. Full observability with OpenTelemetry + Prometheus

**Cost Savings:**
- All Claude: ~$0.50 per file
- Orchestration: ~$0.05 per file (90% savings)

**Trigger Methods:**
- CLI: `uv run python examples/orchestration_test_generation.py --file path/to/file.py`
- GitHub Webhook: POST /generate-tests with file path
- Scheduled: Cron job for new/modified files
"""

import argparse
import asyncio
import logging
import os
import sys
import time
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from tta_dev_primitives.core.base import WorkflowContext
from tta_dev_primitives.integrations import GoogleAIStudioPrimitive
from tta_dev_primitives.observability import get_enhanced_metrics_collector
from tta_dev_primitives.orchestration import (
    DelegationPrimitive,
    MultiModelWorkflow,
    TaskClassifierPrimitive,
)
from tta_dev_primitives.orchestration.delegation_primitive import DelegationRequest
from tta_dev_primitives.orchestration.multi_model_workflow import MultiModelRequest

# Try to import observability integration
try:
    from observability_integration import initialize_observability

    OBSERVABILITY_AVAILABLE = True
except ImportError:
    OBSERVABILITY_AVAILABLE = False

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class TestGenerationWorkflow:
    """Orchestrated workflow for automated test generation.

    **Architecture:**
    - Orchestrator: Claude Sonnet 4.5 (analysis + validation)
    - Executor: Gemini Pro (test generation)
    - Observability: OpenTelemetry + Prometheus

    **Metrics Tracked:**
    - orchestrator_tokens: Tokens used by Claude
    - executor_tokens: Tokens used by Gemini
    - orchestrator_cost: Cost of Claude operations
    - executor_cost: Cost of Gemini operations (always $0.00)
    - total_cost: Total workflow cost
    - cost_savings_vs_all_paid: Percentage saved vs. all-Claude
    - classification: Task complexity (simple/moderate/complex/expert)
    - validation_passed: Whether generated tests passed validation
    """

    def __init__(self) -> None:
        """Initialize test generation workflow."""
        # Initialize observability if available
        if OBSERVABILITY_AVAILABLE:
            success = initialize_observability(
                service_name="test-generation-workflow",
                enable_prometheus=True,
                prometheus_port=9464,
            )
            if success:
                logger.info("✅ Observability initialized (Prometheus on :9464)")
            else:
                logger.warning("⚠️  Observability degraded (OpenTelemetry unavailable)")
        else:
            logger.warning("⚠️  observability_integration not available")

        # Create multi-model workflow
        self.workflow = MultiModelWorkflow(
            executor_primitives={
                "gemini-2.5-pro": GoogleAIStudioPrimitive(
                    model="gemini-2.5-pro", api_key=os.getenv("GOOGLE_API_KEY")
                )
            },
            prefer_free=True,
        )

        # Create delegation primitive for direct delegation
        self.delegation = DelegationPrimitive(
            executor_primitives={
                "gemini-2.5-pro": GoogleAIStudioPrimitive(
                    model="gemini-2.5-pro", api_key=os.getenv("GOOGLE_API_KEY")
                )
            }
        )

        # Metrics collector
        self.metrics_collector = get_enhanced_metrics_collector()

    async def analyze_code(self, file_path: str, code_content: str) -> dict:
        """Analyze code structure (orchestrator role).

        In production, this would be Claude Sonnet 4.5 analyzing the code.
        For demo purposes, we simulate Claude's analysis.

        Args:
            file_path: Path to the code file
            code_content: Content of the code file

        Returns:
            Analysis results with test generation plan
        """
        logger.info(f"🧠 [Orchestrator] Analyzing code: {file_path}")

        # Simulate Claude's analysis (in production, this would be a real LLM call)
        analysis = {
            "file_path": file_path,
            "complexity": "moderate",
            "functions_to_test": self._extract_functions(code_content),
            "test_strategy": "Unit tests with pytest, mock external dependencies",
            "coverage_target": 80,
            "estimated_tokens": 1500,  # Estimated tokens for test generation
        }

        logger.info(
            f"📊 [Orchestrator] Analysis complete: {len(analysis['functions_to_test'])} functions, "
            f"complexity={analysis['complexity']}"
        )

        return analysis

    def _extract_functions(self, code_content: str) -> list[str]:
        """Extract function names from code (simple regex-based extraction)."""
        import re

        # Simple regex to find function definitions
        pattern = r"^\s*(?:async\s+)?def\s+(\w+)\s*\("
        functions = re.findall(pattern, code_content, re.MULTILINE)
        return functions

    async def generate_tests(
        self, file_path: str, code_content: str, analysis: dict, context: WorkflowContext
    ) -> str:
        """Generate tests using Gemini Pro (executor role).

        Args:
            file_path: Path to the code file
            code_content: Content of the code file
            analysis: Analysis results from orchestrator
            context: Workflow context

        Returns:
            Generated test code
        """
        logger.info(f"🤖 [Executor] Generating tests with Gemini Pro...")

        # Create detailed prompt for test generation
        prompt = f"""Generate comprehensive unit tests for the following Python code.

File: {file_path}

Code:
```python
{code_content}
```

Requirements:
- Use pytest framework
- Test all functions: {', '.join(analysis['functions_to_test'])}
- Mock external dependencies
- Aim for {analysis['coverage_target']}% coverage
- Include edge cases and error handling
- Follow best practices for test organization

Generate complete, runnable test code with proper imports and fixtures.
"""

        # Delegate to Gemini Pro
        request = DelegationRequest(
            task_description="Generate unit tests",
            executor_model="gemini-2.5-pro",
            messages=[{"role": "user", "content": prompt}],
            metadata={
                "file_path": file_path,
                "complexity": analysis["complexity"],
                "functions_count": len(analysis["functions_to_test"]),
            },
        )

        response = await self.delegation.execute(request, context)

        logger.info(
            f"✅ [Executor] Tests generated: {len(response.content)} chars, cost=${response.cost}"
        )

        # Record executor metrics
        context.data["executor_tokens"] = response.usage.get("total_tokens", 0)
        context.data["executor_cost"] = response.cost

        return response.content

    async def validate_tests(self, test_code: str, analysis: dict) -> bool:
        """Validate generated tests (orchestrator role).

        In production, this would be Claude Sonnet 4.5 validating the tests.
        For demo purposes, we use simple heuristics.

        Args:
            test_code: Generated test code
            analysis: Analysis results

        Returns:
            True if tests pass validation, False otherwise
        """
        logger.info(f"🔍 [Orchestrator] Validating generated tests...")

        # Simple validation heuristics
        validations = {
            "has_imports": "import pytest" in test_code or "from pytest" in test_code,
            "has_test_functions": "def test_" in test_code,
            "has_assertions": "assert " in test_code,
            "covers_all_functions": all(
                func in test_code for func in analysis["functions_to_test"]
            ),
        }

        passed = all(validations.values())

        logger.info(
            f"{'✅' if passed else '❌'} [Orchestrator] Validation: {sum(validations.values())}/{len(validations)} checks passed"
        )

        return passed

    async def run(self, file_path: str) -> dict:
        """Run the complete test generation workflow.

        Args:
            file_path: Path to the code file

        Returns:
            Workflow results with metrics
        """
        start_time = time.time()

        # Create workflow context
        context = WorkflowContext(
            workflow_id=f"test-gen-{Path(file_path).stem}",
            data={
                "file_path": file_path,
                "workflow_type": "test_generation",
                "orchestrator_model": "claude-sonnet-4.5",
                "executor_model": "gemini-2.5-pro",
            },
        )

        try:
            # Read code file
            with open(file_path, "r") as f:
                code_content = f.read()

            # Step 1: Orchestrator analyzes code
            analysis = await self.analyze_code(file_path, code_content)
            context.data["orchestrator_tokens"] = 200  # Estimated tokens for analysis
            context.data["orchestrator_cost"] = 0.006  # ~$3/1M tokens

            # Step 2: Executor generates tests
            test_code = await self.generate_tests(file_path, code_content, analysis, context)

            # Step 3: Orchestrator validates tests
            validation_passed = await self.validate_tests(test_code, analysis)
            context.data["validation_passed"] = validation_passed
            context.data["orchestrator_tokens"] += 100  # Validation tokens
            context.data["orchestrator_cost"] += 0.003  # Validation cost

            # Calculate total cost and savings
            total_cost = context.data["orchestrator_cost"] + context.data["executor_cost"]
            all_claude_cost = 0.50  # Estimated cost if using Claude for everything
            cost_savings = (all_claude_cost - total_cost) / all_claude_cost

            context.data["total_cost"] = total_cost
            context.data["cost_savings_vs_all_paid"] = cost_savings

            # Calculate duration
            duration_ms = (time.time() - start_time) * 1000

            # Log results
            logger.info("\n" + "=" * 80)
            logger.info("📊 WORKFLOW RESULTS")
            logger.info("=" * 80)
            logger.info(f"File: {file_path}")
            logger.info(f"Functions tested: {len(analysis['functions_to_test'])}")
            logger.info(f"Test code length: {len(test_code)} chars")
            logger.info(f"Validation: {'✅ Passed' if validation_passed else '❌ Failed'}")
            logger.info(f"Duration: {duration_ms:.0f}ms")
            logger.info("\n💰 COST ANALYSIS")
            logger.info(f"Orchestrator (Claude): ${context.data['orchestrator_cost']:.4f}")
            logger.info(f"Executor (Gemini): ${context.data['executor_cost']:.4f}")
            logger.info(f"Total: ${total_cost:.4f}")
            logger.info(f"vs. All-Claude: ${all_claude_cost:.2f}")
            logger.info(f"Cost Savings: {cost_savings*100:.0f}%")
            logger.info("=" * 80)

            # Write test file
            test_file_path = file_path.replace(".py", "_test.py")
            with open(test_file_path, "w") as f:
                f.write(test_code)
            logger.info(f"\n✅ Tests written to: {test_file_path}")

            return {
                "success": True,
                "test_file": test_file_path,
                "validation_passed": validation_passed,
                "metrics": context.data,
                "duration_ms": duration_ms,
            }

        except Exception as e:
            logger.error(f"❌ Workflow failed: {e}")
            return {"success": False, "error": str(e)}


async def main():
    """Main entry point for CLI usage."""
    parser = argparse.ArgumentParser(description="Generate tests using multi-model orchestration")
    parser.add_argument("--file", required=True, help="Path to Python file to generate tests for")
    args = parser.parse_args()

    # Validate file exists
    if not Path(args.file).exists():
        logger.error(f"❌ File not found: {args.file}")
        sys.exit(1)

    # Run workflow
    workflow = TestGenerationWorkflow()
    result = await workflow.run(args.file)

    # Exit with appropriate code
    sys.exit(0 if result["success"] else 1)


if __name__ == "__main__":
    asyncio.run(main())

