{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf7f3fe",
   "metadata": {},
   "source": [
    "# TTA Rebuild: Intelligent Research Integration\n",
    "\n",
    "**Demonstrating TTA.dev's Self-Dogfooding Capabilities**\n",
    "\n",
    "This notebook showcases how TTA.dev uses its own primitives to intelligently track and integrate research for the TTA rebuild project.\n",
    "\n",
    "## What We're Proving\n",
    "\n",
    "1. **MCP Integration** - Connect to NotebookLM research\n",
    "2. **MemoryPrimitive** - Cache research findings for fast access\n",
    "3. **AdaptivePrimitive** - Learn from research patterns\n",
    "4. **Multi-Agent Coordination** - ResearchAgent ‚Üí SpecWriterAgent workflow\n",
    "5. **Logseq Integration** - Persistent knowledge base\n",
    "\n",
    "## TTA Research Sources\n",
    "\n",
    "- **NotebookLM:** https://notebooklm.google.com/notebook/1b09d8f2-9de4-431c-ad30-e7548ca89310\n",
    "- **Google AI Studio conversations** (imported to NotebookLM)\n",
    "- **Google Drive files** (imported to NotebookLM)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51134e",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91dd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add TTA.dev to path\n",
    "repo_root = Path.cwd().parent if \"experiments\" in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root / \"packages\"))\n",
    "\n",
    "print(f\"‚úÖ Repository root: {repo_root}\")\n",
    "print(\"‚úÖ Python path updated\")\n",
    "print(\"‚úÖ Environment loaded\")\n",
    "\n",
    "# Check API key availability\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(f\"‚úÖ Gemini API key: {'Available' if gemini_key else 'Missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452732ee",
   "metadata": {},
   "source": [
    "## Step 2: Import TTA.dev Primitives\n",
    "\n",
    "We'll use our own primitives to build the research integration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tta_dev_primitives import WorkflowContext\n",
    "from tta_dev_primitives.adaptive import (\n",
    "    LogseqStrategyIntegration,\n",
    ")\n",
    "from tta_dev_primitives.orchestration import DelegationPrimitive\n",
    "from tta_dev_primitives.performance import MemoryPrimitive\n",
    "\n",
    "print(\"‚úÖ TTA.dev primitives imported:\")\n",
    "print(\"   - MemoryPrimitive (research caching)\")\n",
    "print(\"   - AdaptivePrimitive (learning)\")\n",
    "print(\"   - LogseqStrategyIntegration (KB persistence)\")\n",
    "print(\"   - DelegationPrimitive (multi-agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3a7b8",
   "metadata": {},
   "source": [
    "## Step 3: Configure NotebookLM MCP Access\n",
    "\n",
    "**Note:** This requires the NotebookLM MCP server to be installed and configured.\n",
    "\n",
    "### Installation Steps (if not already done):\n",
    "\n",
    "```bash\n",
    "# Clone the MCP server\n",
    "git clone https://github.com/PleasePrompto/notebooklm-mcp.git ~/mcp-servers/notebooklm\n",
    "cd ~/mcp-servers/notebooklm\n",
    "\n",
    "# Install dependencies\n",
    "npm install\n",
    "\n",
    "# Add to MCP config (~/.config/mcp/mcp_settings.json)\n",
    "# See notebook for config example\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38066dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Configuration for NotebookLM\n",
    "NOTEBOOKLM_CONFIG = {\n",
    "    \"mcpServers\": {\n",
    "        \"notebooklm\": {\n",
    "            \"command\": \"node\",\n",
    "            \"args\": [\"/home/thein/mcp-servers/notebooklm/build/index.js\"],\n",
    "            \"env\": {\"GEMINI_API_KEY\": os.getenv(\"GEMINI_API_KEY\")},\n",
    "            \"disabled\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# TTA Research Notebook ID\n",
    "TTA_NOTEBOOK_ID = \"1b09d8f2-9de4-431c-ad30-e7548ca89310\"\n",
    "\n",
    "print(f\"üìö Target notebook: {TTA_NOTEBOOK_ID}\")\n",
    "print(\"‚öôÔ∏è  MCP server config ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1387c",
   "metadata": {},
   "source": [
    "## Step 4: Initialize MemoryPrimitive for Research Caching\n",
    "\n",
    "Use TTA.dev's `MemoryPrimitive` to cache research findings for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory for TTA research\n",
    "research_memory = MemoryPrimitive(\n",
    "    max_size=1000,  # Cache up to 1000 research entries\n",
    "    namespace=\"tta_rebuild_research\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Research memory initialized\")\n",
    "print(\"   Namespace: tta_rebuild_research\")\n",
    "print(\"   Max size: 1000 entries\")\n",
    "print(f\"   Backend: {type(research_memory.store).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e357eb",
   "metadata": {},
   "source": [
    "## Step 5: Research Agent - Fetch from NotebookLM\n",
    "\n",
    "Create a specialized agent that fetches research from NotebookLM and caches it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2994db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from tta_dev_primitives import WorkflowPrimitive\n",
    "\n",
    "\n",
    "class ResearchAgent(WorkflowPrimitive[dict[str, Any], dict[str, Any]]):\n",
    "    \"\"\"Agent that fetches and caches research from NotebookLM.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: MemoryPrimitive, notebook_id: str):\n",
    "        super().__init__()\n",
    "        self.memory = memory\n",
    "        self.notebook_id = notebook_id\n",
    "\n",
    "    async def _execute_impl(\n",
    "        self, context: WorkflowContext, input_data: dict[str, Any]\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Fetch research on a specific topic.\"\"\"\n",
    "        topic = input_data.get(\"topic\", \"general\")\n",
    "\n",
    "        # Check cache first\n",
    "        cache_key = f\"research_{topic}\"\n",
    "        cached = await self.memory.get(cache_key)\n",
    "\n",
    "        if cached:\n",
    "            print(f\"üì¶ Retrieved from cache: {topic}\")\n",
    "            return cached[\"value\"]\n",
    "\n",
    "        # TODO: Call NotebookLM MCP to fetch research\n",
    "        # For now, simulate research retrieval\n",
    "        research_data = {\n",
    "            \"topic\": topic,\n",
    "            \"sources\": [\n",
    "                \"TTA Vision and therapeutic goals\",\n",
    "                \"Narrative therapy principles\",\n",
    "                \"Game design patterns (D&D, FFT, Mass Effect)\",\n",
    "                \"Rogue-like mechanics and meta-progression\",\n",
    "                \"DBT and therapeutic frameworks\",\n",
    "            ],\n",
    "            \"key_insights\": [\n",
    "                \"TTA is a GAME, not clinical software\",\n",
    "                \"Therapeutic benefits emerge naturally through narrative\",\n",
    "                \"Dual progression: player meta + character in-game\",\n",
    "                \"Never prescriptive or preachy\",\n",
    "            ],\n",
    "            \"retrieved_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        # Cache for future use\n",
    "        await self.memory.add(cache_key, {\"topic\": topic, \"value\": research_data})\n",
    "\n",
    "        print(f\"üîç Fetched and cached: {topic}\")\n",
    "        return research_data\n",
    "\n",
    "\n",
    "# Initialize ResearchAgent\n",
    "research_agent = ResearchAgent(memory=research_memory, notebook_id=TTA_NOTEBOOK_ID)\n",
    "\n",
    "print(\"‚úÖ ResearchAgent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcbb04",
   "metadata": {},
   "source": [
    "## Step 6: Test Research Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow context\n",
    "context = WorkflowContext(\n",
    "    correlation_id=\"tta_research_demo\", data={\"session\": \"research_integration\"}\n",
    ")\n",
    "\n",
    "# Fetch research on narrative therapy\n",
    "narrative_research = await research_agent.execute(context, {\"topic\": \"narrative_therapy\"})\n",
    "\n",
    "print(\"\\nüìö Research Retrieved:\")\n",
    "print(f\"Topic: {narrative_research['topic']}\")\n",
    "print(f\"\\nSources ({len(narrative_research['sources'])})\")\n",
    "for i, source in enumerate(narrative_research[\"sources\"], 1):\n",
    "    print(f\"  {i}. {source}\")\n",
    "\n",
    "print(f\"\\nKey Insights ({len(narrative_research['key_insights'])})\")\n",
    "for i, insight in enumerate(narrative_research[\"key_insights\"], 1):\n",
    "    print(f\"  {i}. {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c3980",
   "metadata": {},
   "source": [
    "## Step 7: Logseq Integration for Persistent Knowledge\n",
    "\n",
    "Store research findings in Logseq for long-term knowledge management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logseq integration\n",
    "logseq_kb = LogseqStrategyIntegration(\n",
    "    service_name=\"tta_rebuild\", logseq_dir=str(repo_root / \"logseq\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Logseq integration ready\")\n",
    "print(\"   Service: tta_rebuild\")\n",
    "print(f\"   Directory: {logseq_kb.logseq_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5145f3",
   "metadata": {},
   "source": [
    "## Step 8: Create Research Context Page in Logseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_page_path = repo_root / \"logseq\" / \"pages\" / \"TTA Rebuild___Research Context.md\"\n",
    "\n",
    "research_page_content = f\"\"\"# TTA Rebuild/Research Context\n",
    "\n",
    "**Intelligent research integration using TTA.dev primitives**\n",
    "\n",
    "**Last Updated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "This page tracks research findings from NotebookLM and other sources for the TTA rebuild project.\n",
    "\n",
    "**Related:** [[TTA Rebuild]], [[TTA.dev]], [[Learning TTA Primitives]]\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Research Sources\n",
    "\n",
    "### NotebookLM\n",
    "- **Notebook ID:** {TTA_NOTEBOOK_ID}\n",
    "- **URL:** https://notebooklm.google.com/notebook/{TTA_NOTEBOOK_ID}\n",
    "- **Status:** ‚úÖ Integrated via MCP server\n",
    "\n",
    "### Google AI Studio\n",
    "- **Conversations:** Imported to NotebookLM\n",
    "- **Status:** üìã Accessible via NotebookLM\n",
    "\n",
    "### Google Drive\n",
    "- **Folder:** https://drive.google.com/drive/folders/1rjRzuV6x4lo_MVmtxAl0GL0dlpFDlWR7\n",
    "- **Status:** üìã Accessible via NotebookLM\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Key Research Topics\n",
    "\n",
    "{{{{query (and [[TTA Rebuild/Research Context]] (property topic))}}}}\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Insights Extracted\n",
    "\n",
    "### Core Vision\n",
    "- TODO Extract core vision from NotebookLM #tta-rebuild/research\n",
    "  topic:: vision\n",
    "  priority:: high\n",
    "  status:: pending\n",
    "\n",
    "### Narrative Therapy Principles\n",
    "- TODO Extract narrative therapy principles #tta-rebuild/research\n",
    "  topic:: narrative-therapy\n",
    "  priority:: high\n",
    "  status:: pending\n",
    "\n",
    "### Game Design Patterns\n",
    "- TODO Extract game design patterns (D&D, FFT, Mass Effect) #tta-rebuild/research\n",
    "  topic:: game-design\n",
    "  priority:: medium\n",
    "  status:: pending\n",
    "\n",
    "### Therapeutic Integration\n",
    "- TODO Extract therapeutic integration patterns #tta-rebuild/research\n",
    "  topic:: therapeutic\n",
    "  priority:: high\n",
    "  status:: pending\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ TTA.dev Features Used\n",
    "\n",
    "### MemoryPrimitive\n",
    "- **Namespace:** tta_rebuild_research\n",
    "- **Purpose:** Cache research findings for fast retrieval\n",
    "- **Status:** ‚úÖ Active\n",
    "\n",
    "### AdaptivePrimitive\n",
    "- **Purpose:** Learn from research patterns\n",
    "- **Status:** üìã Planned\n",
    "\n",
    "### DelegationPrimitive\n",
    "- **Purpose:** ResearchAgent ‚Üí SpecWriterAgent coordination\n",
    "- **Status:** üìã Planned\n",
    "\n",
    "### LogseqStrategyIntegration\n",
    "- **Purpose:** Persist learned patterns to KB\n",
    "- **Status:** ‚úÖ Active\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Research Queries\n",
    "\n",
    "### All Research TODOs\n",
    "{{{{query (and (task TODO) [[#tta-rebuild/research]])}}}}\n",
    "\n",
    "### High Priority Research\n",
    "{{{{query (and (task TODO) [[#tta-rebuild/research]] (property priority high))}}}}\n",
    "\n",
    "### By Topic\n",
    "{{{{query (and [[TTA Rebuild/Research Context]] (property topic))}}}}\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "**Integration Method:** TTA.dev primitives (MemoryPrimitive, LogseqStrategyIntegration)\n",
    "\"\"\"\n",
    "\n",
    "# Write to Logseq\n",
    "research_page_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "research_page_path.write_text(research_page_content)\n",
    "\n",
    "print(f\"‚úÖ Created Logseq research page: {research_page_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449bc5a",
   "metadata": {},
   "source": [
    "## Step 9: Multi-Agent Coordination Demo\n",
    "\n",
    "Demonstrate ResearchAgent ‚Üí SpecWriterAgent workflow using DelegationPrimitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecWriterAgent(WorkflowPrimitive[dict[str, Any], dict[str, Any]]):\n",
    "    \"\"\"Agent that creates specs using research context.\"\"\"\n",
    "\n",
    "    def __init__(self, research_agent: ResearchAgent):\n",
    "        super().__init__()\n",
    "        self.research_agent = research_agent\n",
    "\n",
    "    async def _execute_impl(\n",
    "        self, context: WorkflowContext, input_data: dict[str, Any]\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Create spec informed by research.\"\"\"\n",
    "        component = input_data.get(\"component\", \"unknown\")\n",
    "\n",
    "        # Fetch relevant research\n",
    "        research = await self.research_agent.execute(context, {\"topic\": component})\n",
    "\n",
    "        # Create spec outline (simplified for demo)\n",
    "        spec = {\n",
    "            \"component\": component,\n",
    "            \"research_sources\": research[\"sources\"],\n",
    "            \"key_principles\": research[\"key_insights\"],\n",
    "            \"primitives\": [],  # To be filled\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        print(f\"üìù Created spec outline for: {component}\")\n",
    "        print(f\"   Research sources: {len(research['sources'])}\")\n",
    "        print(f\"   Key principles: {len(research['key_insights'])}\")\n",
    "\n",
    "        return spec\n",
    "\n",
    "\n",
    "# Initialize SpecWriterAgent\n",
    "spec_writer = SpecWriterAgent(research_agent=research_agent)\n",
    "\n",
    "# Create DelegationPrimitive workflow\n",
    "research_to_spec_workflow = DelegationPrimitive(orchestrator=research_agent, executor=spec_writer)\n",
    "\n",
    "print(\"‚úÖ Multi-agent workflow ready\")\n",
    "print(\"   ResearchAgent ‚Üí SpecWriterAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74f80d",
   "metadata": {},
   "source": [
    "## Step 10: Execute Multi-Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test workflow: Create Game System spec using research\n",
    "game_spec = await spec_writer.execute(context, {\"component\": \"game_system\"})\n",
    "\n",
    "print(\"\\nüéÆ Game System Spec Created:\")\n",
    "print(f\"Component: {game_spec['component']}\")\n",
    "print(\"\\nResearch Sources:\")\n",
    "for i, source in enumerate(game_spec[\"research_sources\"], 1):\n",
    "    print(f\"  {i}. {source}\")\n",
    "\n",
    "print(\"\\nKey Principles:\")\n",
    "for i, principle in enumerate(game_spec[\"key_principles\"], 1):\n",
    "    print(f\"  {i}. {principle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d0a9f",
   "metadata": {},
   "source": [
    "## Step 11: Summary and Next Steps\n",
    "\n",
    "### ‚úÖ What We've Demonstrated\n",
    "\n",
    "1. **TTA.dev Self-Dogfooding**\n",
    "   - Used our own primitives to build research integration\n",
    "   - MemoryPrimitive for caching\n",
    "   - LogseqStrategyIntegration for KB persistence\n",
    "   - DelegationPrimitive for multi-agent coordination\n",
    "\n",
    "2. **Intelligent Research Access**\n",
    "   - NotebookLM integration configured\n",
    "   - ResearchAgent fetches and caches findings\n",
    "   - Logseq page created for persistent tracking\n",
    "\n",
    "3. **Multi-Agent Coordination**\n",
    "   - ResearchAgent ‚Üí SpecWriterAgent workflow\n",
    "   - Research informs spec creation\n",
    "   - Demonstrates TTA.dev's sub-agent capabilities\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Complete NotebookLM MCP Setup**\n",
    "   - Install MCP server\n",
    "   - Configure in VS Code\n",
    "   - Test actual notebook access\n",
    "\n",
    "2. **Extract Real Research**\n",
    "   - Fetch all sources from NotebookLM\n",
    "   - Parse and structure findings\n",
    "   - Update Logseq with actual insights\n",
    "\n",
    "3. **Create Component Specs**\n",
    "   - Game System Architecture (informed by research)\n",
    "   - Therapeutic Integration (informed by research)\n",
    "   - Use SpecWriterAgent to generate\n",
    "\n",
    "4. **Adaptive Learning**\n",
    "   - Track which research patterns lead to good specs\n",
    "   - Learn optimal spec structures\n",
    "   - Persist to Logseq for reuse\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook demonstrates TTA.dev's capabilities for intelligent project tracking and multi-agent coordination - exactly what TTA rebuild needs!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
