{"pattern": "adaptive_retry", "antipattern": "# Manual tuning of retry parameters\nfor endpoint in endpoints:\n    retry_config = tune_retry_params(endpoint)  # Manual process\n    retry_primitive = RetryPrimitive(**retry_config)", "correct": "from tta_dev_primitives.adaptive import AdaptiveRetryPrimitive, LogseqStrategyIntegration, LearningMode\n\n# Self-improving retry\nlogseq = LogseqStrategyIntegration(\"api_service\")\nadaptive_retry = AdaptiveRetryPrimitive(\n    target_primitive=api_call,\n    logseq_integration=logseq,\n    enable_auto_persistence=True,\n    learning_mode=LearningMode.ACTIVE\n)\n\n# Learning happens automatically\nresult = await adaptive_retry.execute(data, context)", "explanation": "Use AdaptiveRetryPrimitive for automatic parameter tuning. Learns optimal retry strategies from execution patterns.", "severity": "info", "rule": "TTA_ADAPTIVE"}
{"pattern": "memory_workflow", "antipattern": "# Manual conversation history management\nconversation_history = []\n\nasync def chat(user_input: str) -> str:\n    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n    response = await llm_with_history(conversation_history)\n    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n    return response", "correct": "from tta_dev_primitives.performance import MemoryPrimitive\n\n# Zero-setup conversational memory\nmemory = MemoryPrimitive(max_size=100)\n\nasync def chat(user_input: str) -> str:\n    # Store user message\n    await memory.add(f\"user_{timestamp}\", {\"role\": \"user\", \"content\": user_input})\n    \n    # Search relevant history\n    history = await memory.search(keywords=user_input.split()[:3])\n    \n    # Generate with context\n    response = await llm_generate(user_input, history)\n    \n    # Store response\n    await memory.add(f\"assistant_{timestamp}\", {\"role\": \"assistant\", \"content\": response})\n    return response", "explanation": "Use MemoryPrimitive for conversational memory. Benefits: zero setup, LRU eviction, search, optional Redis upgrade.", "severity": "warning", "rule": "TTA_MEMORY"}
{"pattern": "mixed_composition", "antipattern": "async def complex_workflow(data: dict) -> dict:\n    # Sequential then parallel manually\n    preprocessed = await preprocess(data)\n    \n    # Manual parallel execution\n    branch_results = await asyncio.gather(\n        branch1(preprocessed),\n        branch2(preprocessed),\n        branch3(preprocessed)\n    )\n    \n    # Sequential again\n    aggregated = await aggregate(branch_results)\n    return await postprocess(aggregated)", "correct": "from tta_dev_primitives import WorkflowContext\n\n# Mix >> and | operators\nworkflow = (\n    preprocess >>\n    (branch1 | branch2 | branch3) >>\n    aggregate >>\n    postprocess\n)\n\ncontext = WorkflowContext()\nresult = await workflow.execute(data, context)", "explanation": "Mix sequential (>>) and parallel (|) operators for complex workflows. Automatic orchestration with tracing.", "severity": "error", "rule": "TTA001"}
{"pattern": "instrumented_primitive", "antipattern": "from tta_dev_primitives import WorkflowPrimitive\n\nclass MyPrimitive(WorkflowPrimitive[dict, dict]):\n    # No observability configured\n    async def _execute_impl(self, context, input_data):\n        return result", "correct": "from tta_dev_primitives.observability import InstrumentedPrimitive\n\nclass MyPrimitive(InstrumentedPrimitive[dict, dict]):\n    \"\"\"Primitive with automatic observability.\"\"\"\n    \n    async def _execute_impl(self, context, input_data):\n        # Automatic span creation, metrics, logging\n        return result", "explanation": "Extend InstrumentedPrimitive for automatic observability. Benefits: OpenTelemetry spans, metrics, structured logging.", "severity": "warning", "rule": "TTA_OBSERVABILITY"}
{"pattern": "e2b_validation", "antipattern": "# Generate code without validation\ngenerated_code = await llm_generate_code(requirement)\nreturn generated_code  # Hope it works!", "correct": "from tta_dev_primitives.integrations import CodeExecutionPrimitive\n\n# Iterative validation pattern\ncode_executor = CodeExecutionPrimitive()\n\nfor attempt in range(3):\n    # Generate code\n    code = await llm_generate_code(requirement, previous_errors)\n    \n    # Execute in sandbox\n    result = await code_executor.execute({\"code\": code, \"timeout\": 30}, context)\n    \n    if result[\"success\"]:\n        return {\"code\": code, \"output\": result[\"logs\"]}\n    \n    # Feed error back for next iteration\n    previous_errors = result[\"error\"]", "explanation": "Use CodeExecutionPrimitive for iterative code validation. Generate -> Execute -> Fix -> Repeat until working.", "severity": "info", "rule": "TTA_E2B"}
{"pattern": "context_propagation", "antipattern": "# Missing correlation IDs\nasync def service_a(data: dict) -> dict:\n    result = await service_b(data)  # Lost trace context\n    return result", "correct": "from tta_dev_primitives import WorkflowContext\n\n# Context propagates automatically\nworkflow = service_a >> service_b >> service_c\n\ncontext = WorkflowContext(\n    correlation_id=\"req-123\",\n    data={\"user_id\": \"user-789\", \"request_type\": \"analysis\"}\n)\n\nresult = await workflow.execute(data, context)\n# Context and correlation_id propagate through entire workflow", "explanation": "WorkflowContext automatically propagates correlation IDs and metadata. Essential for distributed tracing.", "severity": "error", "rule": "TTA002"}
{"pattern": "error_recovery", "antipattern": "try:\n    result = await unreliable_operation(data)\nexcept Exception as e:\n    logger.error(f\"Failed: {e}\")\n    # Now what? No recovery strategy", "correct": "from tta_dev_primitives.recovery import RetryPrimitive, FallbackPrimitive\n\n# Layered recovery strategy\nworkflow = (\n    RetryPrimitive(\n        primitive=unreliable_operation,\n        max_retries=3,\n        backoff_strategy=\"exponential\"\n    ) >>\n    FallbackPrimitive(\n        fallbacks=[backup_operation, cached_response]\n    )\n)\n\n# Automatic retry then fallback\nresult = await workflow.execute(data, context)", "explanation": "Layer RetryPrimitive and FallbackPrimitive for comprehensive recovery. Retry first, then fallback.", "severity": "error", "rule": "TTA003"}
{"pattern": "cost_optimization", "antipattern": "# Every call hits expensive API\nasync def analyze(data: dict) -> dict:\n    return await gpt4_call(data)  # $0.03 per call", "correct": "from tta_dev_primitives.performance import CachePrimitive\nfrom tta_dev_primitives.core import RouterPrimitive\n\n# Cache + Router for 70-80% cost reduction\nworkflow = (\n    CachePrimitive(ttl_seconds=3600) >>  # 40-60% reduction\n    RouterPrimitive(\n        routes={\"fast\": gpt4_mini, \"quality\": gpt4},  # 30-40% additional\n        router_fn=lambda d, c: \"fast\" if simple(d) else \"quality\"\n    )\n)\n\nresult = await workflow.execute(data, context)", "explanation": "Combine CachePrimitive and RouterPrimitive for cost optimization. Typical savings: 70-80% on LLM costs.", "severity": "warning", "rule": "TTA005"}
{"pattern": "testing", "antipattern": "# Complex mocking setup\nfrom unittest.mock import Mock, patch\n\n@patch('module.external_api')\nasync def test_workflow(mock_api):\n    mock_api.return_value = {\"result\": \"test\"}\n    # Complex mock configuration", "correct": "from tta_dev_primitives.testing import MockPrimitive\nimport pytest\n\n@pytest.mark.asyncio\nasync def test_workflow():\n    # Simple mock primitive\n    mock_api = MockPrimitive(return_value={\"result\": \"test\"})\n    \n    workflow = step1 >> mock_api >> step3\n    result = await workflow.execute(data, context)\n    \n    assert mock_api.call_count == 1", "explanation": "Use MockPrimitive for testing workflows. Simpler than unittest.mock, integrates with primitives.", "severity": "info", "rule": "TTA_TESTING"}
{"pattern": "type_safety", "antipattern": "class MyPrimitive(WorkflowPrimitive):\n    # Missing type parameters\n    async def _execute_impl(self, context, input_data):\n        return result", "correct": "from tta_dev_primitives import WorkflowPrimitive, WorkflowContext\n\nclass MyPrimitive(WorkflowPrimitive[InputModel, OutputModel]):\n    \"\"\"Type-safe primitive.\"\"\"\n    \n    async def _execute_impl(\n        self,\n        context: WorkflowContext,\n        input_data: InputModel\n    ) -> OutputModel:\n        # Full type safety\n        return result", "explanation": "Use type parameters in WorkflowPrimitive[TInput, TOutput] for type safety. Enables better IDE support.", "severity": "warning", "rule": "TTA_TYPES"}
